---
title: 私有化部署Llama
author: mmy83
date: 2024-05-09 19:57:00 +0800
categories: [IT技术, AI]
tags: [AI, 人工智能, Llama, ollama, 私有化部署]
math: true
mermaid: true
image:
  path: /images/2024-05-09/私有化部署Llama/私有化部署Llama-00.png
  lqip: data:image/webp;base64,UklGRmwAAABXRUJQVlA4IGAAAACQAgCdASoIAAUAAUAmJbACdLoAyQDJAAUPCRhQAAD+9f+E1oQlxxjaDOx/iu6WCxZc1Fn+ewKv3hwSjoi//EqM/OgnbxCfGg0ehvY1/5FCSA9b5JY+YD0b9b4P+TdmQAA=
  alt: 私有化部署Llama
---

## 开篇

&emsp;&emsp;最近一直研究各种AI，也尝试了很多，今天想自己深入研究一下，从在 Ubuntu 上私有化部署一个AI开始。

## Ollama

&emsp;&emsp;Ollama 是一个本地部署的大模型运行框架，它提供了一个简单的命令行工具来帮助用户在本地运行大模型。Ollama 支持多个大模型，包括 运行Llama 3、Phi 3、Mistral、Gemma和其他型号和定制并创建您自己的模型。Ollama 支持 macOS、Linux 和 Windows。

### 安装

```shell
curl -fsSL https://ollama.com/install.sh | sh
```

> macOS、 Windows 上安装只需要下载对应版本安装即可。
{: .prompt-tip }

### 命令

```console
root@mmy83:~# ollama -h
Large language model runner

Usage:
  ollama [flags]
  ollama [command]

Available Commands:
  serve       Start ollama
  create      Create a model from a Modelfile
  show        Show information for a model
  run         Run a model
  pull        Pull a model from a registry
  push        Push a model to a registry
  list        List models
  cp          Copy a model
  rm          Remove a model
  help        Help about any command

Flags:
  -h, --help      help for ollama
  -v, --version   Show version information

Use "ollama [command] --help" for more information about a command.
```

## 快速开始

```shell
ollama run llama3
```

&emsp;&emsp;漫长的等待之后，安装成功了，对的，安装成功了！而且这时候还可以直接通过命令行进行交互。但是这个交互界面并不友好，所有需要一个友好的界面。

## 安装 Open WebUI

```shell
# 需要先安装docker，并且和ollama在同一台机器上
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

## 问题解决

&emsp;&emsp;安装 Open WebUI 后，发现无法连接到ollama，原因很简单，因为ollama默认只能127.0.0.1访问，所以需要修改一下配置。

```shell
# 为了允许其他服务器访问ollama服务，需要将host设置成0.0.0.0，修改ollama的配置文件
vim /etc/systemd/system/ollama.service
```

```console
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment=“PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin”
# 增加这一行
Environment=“OLLAMA_HOST=0.0.0.0”

[Install]
WantedBy=default.target
```

```shell
# 重启ollama
systemctl daemon-reload
systemctl restart ollama
```

## 仓库

&emsp;&emsp;Ollama支持[Ollama.com/library](https://ollama.com/library)上提供的型号列表，也可以通过```ollama create```命令创建自己的模型。

&emsp;&emsp;以下是一些可以下载的示例模型：

|Model|	Parameters|	Size|	Download|
|:---|:---|:---|:---|
|Llama 3|	8B|	4.7GB	|ollama run llama3|
|Llama 3|	70B|	40GB|	ollama run llama3:70b|
|Phi-3|	3.8B|	2.3GB	|ollama run phi3|
|Mistral	|7B	|4.1GB	|ollama run mistral|
|Neural Chat|	7B	|4.1GB	|ollama run neural-chat|
|Starling|	7B	|4.1GB	|ollama run starling-lm|
|Code Llama	|7B	|3.8GB	|ollama run codellama|
|Llama 2 Uncensored	|7B	|3.8GB	|ollama run llama2-uncensored|
|LLaVA|7B	|4.5GB	|ollama run llava|
|Gemma|2B	|1.4GB	|ollama run gemma:2b|
|Gemma	|7B	|4.8GB	|ollama run gemma:7b|
|Solar	|10.7B	|6.1GB	|ollama run solar|

## 创建模型

&emsp;&emsp;ollama create 命令可以创建自己的模型，需要指定模型名称和模型文件，Ollama支持在Modelfile中导入GGUF模型。

### 下载

&emsp;&emsp;Ollama支持在Modelfile中导入GGUF模型,从[https://huggingface.co/zhouzr/Llama3-8B-Chinese-Chat-GGUF/tree/main](https://huggingface.co/zhouzr/Llama3-8B-Chinese-Chat-GGUF/tree/main)下载模型文件。

![下载模型文件](/images/2024-05-09/私有化部署Llama/私有化部署Llama-01.png)

### Modelfile

`````shell
{% raw %}
FROM ./Llama3-8B-Chinese-Chat.q6_k.GGUF
TEMPLATE """{{ if .System }}<|start_header_id|>system<|end_header_id|>

{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>

{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>

{{ .Response }}<|eot_id|>"""
PARAMETER stop "<|start_header_id|>"
PARAMETER stop "<|end_header_id|>"
PARAMETER stop "<|eot_id|>"
PARAMETER stop "<|reserved_special_token"
{% endraw %}
`````

### 创建

```shell
ollama create Llama3-8B-Chinese-Chat-q6 -f Modelfile
ollama run Llama3-8B-Chinese-Chat-q6
```
